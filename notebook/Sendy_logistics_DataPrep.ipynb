{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SENDY LOGISTICS DATA PREPROCESING \n",
    "---\n",
    "1. Make your **1st line a comment**, just to give clarity to the Team about the task/code used.\n",
    "2. If you happen to get a **CODE snippet** from **stackoverflow/Blog**, Make your **2nd line the link referencing the code/post**. for later reference if team members need clarity.\n",
    "\n",
    "---\n",
    "## HEADS UP\n",
    "*The following steps will serve as a guide-line not mandatory step and they might not be in order.*\n",
    "\n",
    "- split data into subsets (train & validation/test)\n",
    "- data preparation\n",
    "    - imputing misssing values\n",
    "    - Changing Data Types (if necessary e.g [df.convert_dtypes()](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.convert_dtypes.html)| [df.astype()](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.astype.html) based features [pd.to_datetime(df['date'])](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.to_datetime.html?highlight=to_datetime))\n",
    "    - One Hot Encoding [more](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html)\n",
    "    - Ordinal Encoding [more](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OrdinalEncoder.html#sklearn.preprocessing.OrdinalEncoder)\n",
    "    - Target Encording (Do more Research)\n",
    "    - Target Transformation Regressor (Do [more Research](https://scikit-learn.org/stable/modules/generated/sklearn.compose.TransformedTargetRegressor.html?highlight=transform#sklearn.compose.TransformedTargetRegressor) )\n",
    "        - Target might be transformed when linear algorithms such as Linear Regression, etc.\n",
    "- [Scale](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html?highlight=scale#sklearn.preprocessing.StandardScaler) & transform \n",
    "- feature Engineering (read more)\n",
    "    - Feature Interaction\n",
    "    - Bin Numeric Features\n",
    "    - Trigonometry Features\n",
    "    - Group Features\n",
    "    - Polynomial Features\n",
    "    - Combine Rare Levels\n",
    "- feature Selection (AKA variables selection)\n",
    "    - SelectKBest, SelectFromModel, RFE [more here](https://scikit-learn.org/stable/search.html?q=feature+selection) \n",
    "    - Feature Importance\n",
    "    - collinearity (Remove Multicollinearity using threshold >= 0.9, )\n",
    "        - rules: if feature A is highly correlated with feature B but A is less corelated with Target, then Remove Feature A\n",
    "    - Principal Component Analysis (PCA)\n",
    "    - VarianceThreshold [view](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.VarianceThreshold.html?highlight=feature%20selection#sklearn.feature_selection.VarianceThreshold)\n",
    "        - SET Threshold and Ignore Low Variance fearues below the Threshold.\n",
    "- Clustering (Removing Outliers / creating clusters)\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Library Imports\n",
    "---\n",
    "Keep it clean, import Libraries at the Top!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data manipulation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# data visualization\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "\n",
    "# Pipeline\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.compose import make_column_transformer\n",
    "\n",
    "# preprocessing\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# More imports Below\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Import Datasets\n",
    "---\n",
    "- By Default the notebook is using the github links to fetch data remotely, change to local if need be. \n",
    "> e.g replace URL_TRAIN with URL_local_TRAIN\n",
    "- DO NOT FORGET TO CHANGE BACK THE LINKS BEFORE CREATING A PULL REQUEST\n",
    "> Use The following Links for Local Machine NoteBooks/Jupyterlab (this will save data)\n",
    ">This assumes the notebook.ipynb is inside the Notebooks folder\n",
    "\n",
    "```python\n",
    "\n",
    "    URL_local_TRAIN = \"/data/Train.csv\"\n",
    "    URL_local_TEST = \"/data/Test.csv\"\n",
    "    URL_local_RIDERS = \"/data/Riders.csv\" \n",
    "    URL_local_DD = \"/data/VariableDefinitions.csv\"\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONSTANTS\n",
    "\n",
    "URL_TRAIN = \"https://raw.githubusercontent.com/Explore-EDSA-2020/Sendy-Logistics-Challenge/master/data/Train.csv\"\n",
    "URL_TEST = \"https://raw.githubusercontent.com/Explore-EDSA-2020/Sendy-Logistics-Challenge/master/data/Test.csv\"\n",
    "URL_RIDERS = \"https://raw.githubusercontent.com/Explore-EDSA-2020/Sendy-Logistics-Challenge/master/data/Riders.csv\" \n",
    "URL_DD = \"https://raw.githubusercontent.com/Explore-EDSA-2020/Sendy-Logistics-Challenge/master/data/VariableDefinitions.csv\" # Data Dictionary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading the data to dataframe\n",
    "\n",
    "train_df  = pd.read_csv(URL_TRAIN)\n",
    "test_df   = pd.read_csv(URL_TEST)\n",
    "riders_df = pd.read_csv(URL_RIDERS)\n",
    "data_dictionary_df = pd.read_csv(URL_DD)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train without riders:  (21201, 29)\n",
      "train merged with riders:  (21201, 33)\n",
      "---------------------------------------\n",
      "test without riders:  (7068, 25)\n",
      "test merged with riders:  (7068, 29)\n"
     ]
    }
   ],
   "source": [
    "# making a copy of the data to avoid altering the original data\n",
    "train_riders = train_df.copy()\n",
    "test_riders  = test_df.copy()\n",
    "\n",
    "# merged train with riders\n",
    "train_riders = train_riders.merge(riders_df, how='left', on='Rider Id')\n",
    "test_riders  = test_riders.merge(riders_df, how='left', on='Rider Id')\n",
    "\n",
    "# view dimension\n",
    "print('train without riders: ', train_df.shape)\n",
    "print('train merged with riders: ', train_riders.shape)\n",
    "print('---------------------------------------')\n",
    "print('test without riders: ', test_df.shape)\n",
    "print('test merged with riders: ', test_riders.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Data Preprocessing\n",
    "---\n",
    "Avoid imputing missing values from test data, instead use a pipeline so we can inherit the **train properties** e.g mean, median, mode(most common) or constant: \"missing\". **Ask why in the upcoming meeting!**\n",
    "\n",
    "---\n",
    "You take it from here!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
